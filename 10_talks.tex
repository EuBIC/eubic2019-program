\mysection{6}{\color{eubicRed} Talk Abstracts}

\noindent\textbf{Day 2 -- 11. January 2017}
\addcontentsline{toc}{subsection}{Day 2}

\subsubsection*{\color{eubicRed} Constructing community knowledge for peptide identification and quantification}
{\color{eubicGray}Nuno Bandeira}

Over 95\% of proteomics mass spectrometry data in the public domain does not
have proper peptide or protein identifications; the situation is even worse when
it comes to quantification. The bioinformatics challenges before us thus begin
with i) systematic reanalysis of all public data for discovery of new protein
variants and post-translational modifications and ii) principled aggregation of
billions of search results into a community-wide spectral library reusable for
peptide identification and quantification in both DDA and DIA experiments. We
will present recent developments in these areas and discuss ways to tackle these
challenges by incorporating contributions from multiple research groups.


\subsubsection*{\color{eubicRed} Broadening the mission: quality assessment for quantitative and label-dependent mass spectrometry}
{\color{eubicGray}David Tabb}

In April of 2016, the HUPO-PSI created a new working group devoted to developing
infrastructure for quality control (QC) of proteomic and metabolomic data sets.
A quick survey of the QC literature would suggest that label-free,
data-dependent LC-MS/MS has drawn almost attention in this field. A closer look,
though, finds interesting QC advances and opportunities in other types of
experiments. This talk will examine QC implementations in Spectrum Mill,
SProCoP, and MSstats for isobaric labels like iTRAQ as well as targeted
quantitation experiments. Finally we will look at some potential avenues for
assessing quality in Data-Independent Acquisition and MALDI experiment designs.


\subsubsection*{\color{eubicRed} A discovery portal for bioinformatics resources}
{\color{eubicGray}ELIXIR}

ELIXIR (1) the infrastructure for biological information, is building a
discovery portal (2) for bioinformatics resources world-wide, built upon a
distributed, community curation of a wide range of key resources, including
analytical tools and data services. At the heart of this effort is a registry of
essential scientific and technical information, conforming to a consistent
syntactic and semantic standard. Convenient tools help an end-user to find,
understand and compare the resources they need, and to use and connect them.
The registry will also make software more citable, as PubMed does for scientific
publications. ELIXIR is supporting resource providers, including research
infrastructures, institutes, projects and individuals, to contribute to an
emerging global distributed curation effort, that will ensure comprehensive
content and high quality annotations, both of which are essential for the
sustainable impact of the registry in the community. The registry will, in due
course, expose the results of tool benchmarking and service monitoring to
provide the end-user with a robust, scientifically relevant measure of quality
and performance. Furthermore, integration of the registry with key workbench
environments will assist the users with managing their day-to-day workflows.
To get involved in this effort please contact the registry coordinators (3).

\noindent
(1) \url{http://www.elixir-europe.org/}\\
(2) \url{https://bio.tools}\\
(3) registry@elixir-dk.org


\subsubsection*{\color{eubicRed} Developing structural interactomics and its application in cell biology}
{\color{eubicGray}Fan Liu}

In the last decade, chemical cross-linking combined with mass spectrometry
(XL-MS) has become an increasingly powerful approach to probe the in-solution
structures of proteins/protein assemblies. In principle, XL-MS is able to
profile the structure of individual proteins, topological maps of protein
assemblies and protein interaction networks. However, its current scope is
mainly limited to the characterization of endogenously purified or in vitro
reconstituted protein assemblies. Therefore, to comprehensively understand the
protein interactome, the XL-MS must move beyond simple protein complexes to the
proteome-wide analysis of interaction patterns in vivo. Accordingly, innovative
methods are urgently needed to overcome the technical limitations in the field.

Here, we describe a major leap forward in cross-linking mass spectrometry
demonstrating a new integrated workflow that robustly identifies cross-links
from proteome samples. Our approach is based on the application of a cleavable
cross-linker, sequential CID and ETD MS2 acquisitions of peptide fragmentation
spectra of cross-linked peptides, and a dedicated search engine termed XlinkX.
We applied this novel XL-MS strategy to several highly complex samples,
including E. coli lysate, HeLa lysate and intact mitochondria. Through thousands
to ten thousands of cross-links, were obtained crucial insights into the
interaction patterns, the binding interfaces and the molecular organization of
the protein components, allowing us to gain a deeper understanding of various
functional processes of the cell.

\vspace{1cm}
\noindent\textbf{Day 3 -- 12. January 2017}
\addcontentsline{toc}{subsection}{Day 3}

\subsubsection*{\color{eubicRed} Open Data}
{\color{eubicGray}Lennart Martens}

To hell with the problems with getting data open (that battle has been won) -
why are we not doing more with the data that are there yet, and how do we fix
this.


\subsubsection*{\color{eubicRed} Issues of LC-MS Quantification}
{\color{eubicGray}Oliver Kohlbacher}



\subsubsection*{\color{eubicRed} Reactome: A curated knowledgebase of biomolecular pathways}
{\color{eubicGray}Antonio Fabregat Mundo}

Reactome (\url{http://www.reactome.org}) is a free, open-source, curated and
peer-reviewed knowledge base of biomolecular pathways, aiming to provide
intuitive bioinformatics tools for visualisation, interpretation and analysis of
pathway knowledge and to support basic research, genome analysis, modelling,
systems biology and education. Pathways are built from connected "reactions"
that encompass many types of biochemical events. Reactions are derived from
literature and must cite a publication that experimentally validates them.
Pathways are authored by expert biologists and peer reviewed before
incorporation into the database. In its latest release (v58), Reactome
includes 10,168 reactions covering 10,212 human gene products and supported by
24,968 literature references. Users can search for proteins or compounds and see
details of the complexes, reactions and pathways they participate in. Pathway
diagrams allow users to examine the molecular events that constitute the steps
in pathways and to view details of the proteins, complexes and compounds
involved. Different forms of pathways analysis can be performed with the
Reactome analysis tools. Users can submit a list of identifiers for
overrepresentation analysis or submit quantitative datasets, such as microarray
data, for expression analysis. Results of these analyses are overlaid onto the
Pathways Overview and Diagram Viewer for easy navigation and interpretation.
Interaction data from multiple resources can be used to expand pathways.
Interactors from IntAct are included by default in the search feature and can be
taken into account in the analysis service. Finally, pathways or all Reactome
content can be downloaded in many formats including TSV, CSV, PDF, SBML, BioPax
and PSI-MITAB.



\vspace{1cm}
\noindent\textbf{Day 4 -- 13. January 2017}
\addcontentsline{toc}{subsection}{Day 4}

\subsubsection*{\color{eubicRed} The MaxQuant and Perseus Computational Platforms for Comprehensive Analysis of Large-scale\\ (Prote)Omics Data}
{\color{eubicGray}J\"urgen Cox}

Currently, a main bottleneck in proteomics is the downstream biological analysis
of highly multivariate quantitative protein abundance data. It will be shown how
the Perseus software supports researchers in interpreting protein
quantification, interaction and posttranslational modification data. A
comprehensive portfolio of statistical tools for high-dimensional omics data
analysis is contained covering normalization, pattern recognition, time series
analysis, cross-omics comparisons and multiple hypothesis testing. A machine
learning module supports classification and validation of patient groups for
diagnosis and prognosis, also detecting predictive protein signatures. Central
to Perseus is a user-friendly, interactive workflow environment providing
complete documentation of computational methods used in a publication. All
activities in Perseus are realized as plugins and users can extend the software
by programming their own, which can be shared through a plugin store. Perseus
combines a powerful arsenal of algorithms with intuitive usability by biomedical
domain experts, making it suitable for interdisciplinary analysis of complex
large datasets.


\subsubsection*{\color{eubicRed} Integrating Data and Software using Ontologies}
{\color{eubicGray}Magnus Palmblad}

In a general sense, we can structure data in two ways: top-down (a priori,
supervised) or bottom-up (a posteriori, unsupervised). In the former, we decide
and impose the categories and hierarchies on the data. In the latter, we let the
data speak for itself and use machine learning or statistical methods to
generate a data-dependent structure.

For most purposes of multi-omics integration of heterogeneous data, a strong
case can be made for a supervised system for formal naming and definition of
data categories (types), properties and relationships within our domain. We call
such a system an ontology. In bioinformatics, we use different ontologies for
different purposes. We describe functions or cellular localizations of proteins
using the Gene Ontology (GO). To integrate anatomically defined data involving a
single species, we use a species-specific anatomical ontology such as ZFA (for
zebrafish) or MA (for mouse). To compare data across model systems and the
human, we can use the generic vertebrate UBERON. To provide metadata describing
how our mass spectra were acquired, we use the PSI-MS controlled vocabulary. And
for categorizing bioinformatics software tools themselves, we use the EDAM
ontology in bio.tools.

In this talk I will gently introduce these concepts and illustrate how we use
ontologies in our everyday work.
